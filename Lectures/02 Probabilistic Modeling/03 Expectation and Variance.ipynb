{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation and variance are two fundamental concepts in probability theory and statistics that provide important insights into the behavior of random variables. These concepts are crucial for understanding and characterizing the distribution of a random variable, as well as for making informed decisions based on probabilistic models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expectation**, also known as the expected value or mean, is a measure of the central tendency of a random variable. It represents the average value of the random variable over a large number of trials or observations. Expectation is essential for understanding the typical behavior of a random variable and for making predictions based on probabilistic models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance**, on the other hand, is a measure of the dispersion or spread of a random variable around its expected value. It quantifies how much the values of the random variable deviate from the mean. A high variance indicates that the values are spread out over a wide range, while a low variance suggests that the values are clustered closely around the mean. Variance is crucial for assessing the uncertainty or variability associated with a random variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, expectation and variance provide a comprehensive description of a random variable's behavior. They are used extensively in various fields, including machine learning, data analysis, and financial modeling, to make informed decisions based on probabilistic models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will delve deeper into the concepts of expectation and variance, exploring their definitions, properties, and formulas for both discrete and continuous random variables. We will also discuss related concepts such as standard deviation, covariance, and correlation, and explore their applications in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this lecture, you will have a solid understanding of expectation and variance, enabling you to effectively analyze and interpret the behavior of random variables in various contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Expectation](#toc1_)    \n",
    "  - [Definition of expectation (expected value)](#toc1_1_)    \n",
    "  - [Notation and formula for discrete and continuous random variables](#toc1_2_)    \n",
    "  - [Properties of expectation](#toc1_3_)    \n",
    "  - [Examples and calculations](#toc1_4_)    \n",
    "- [Variance](#toc2_)    \n",
    "  - [Definition of variance](#toc2_1_)    \n",
    "  - [Notation and formula for discrete and continuous random variables](#toc2_2_)    \n",
    "  - [Properties of variance](#toc2_3_)    \n",
    "  - [Examples and calculations](#toc2_4_)    \n",
    "- [Standard Deviation](#toc3_)    \n",
    "  - [Definition of standard deviation](#toc3_1_)    \n",
    "  - [Relationship between variance and standard deviation](#toc3_2_)    \n",
    "  - [Interpreting standard deviation](#toc3_3_)    \n",
    "- [Covariance and Correlation](#toc4_)    \n",
    "  - [Definition of covariance](#toc4_1_)    \n",
    "  - [Formula for calculating covariance](#toc4_2_)    \n",
    "  - [Definition of correlation](#toc4_3_)    \n",
    "  - [Formula for calculating correlation](#toc4_4_)    \n",
    "  - [Interpreting covariance and correlation](#toc4_5_)    \n",
    "- [Applications in Machine Learning](#toc5_)    \n",
    "  - [Role of expectation and variance in machine learning](#toc5_1_)    \n",
    "  - [Examples of how these concepts are used in various algorithms](#toc5_2_)    \n",
    "- [Summary](#toc6_)    \n",
    "  - [Recap of key points](#toc6_1_)    \n",
    "  - [Importance of understanding expectation and variance in machine learning](#toc6_2_)    \n",
    "  - [Resources for further study](#toc6_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Expectation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation, also known as the expected value or mean, is a fundamental concept in probability theory that represents the average value of a random variable over a large number of trials or observations. It is a measure of the central tendency of a random variable and provides insight into its typical behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Definition of expectation (expected value)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value of a random variable $X$, denoted as $E(X)$ or $\\mu$, is the sum of the products of each possible value of $X$ and its corresponding probability. In other words, it is the weighted average of the possible values, where the weights are the probabilities of each value occurring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Notation and formula for discrete and continuous random variables](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a discrete random variable $X$ with probability mass function $P(X = x)$, the expected value is given by:\n",
    "\n",
    "$E(X) = \\sum_{x} x P(X = x)$\n",
    "\n",
    "where the sum is taken over all possible values of $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a continuous random variable $X$ with probability density function $f(x)$, the expected value is given by:\n",
    "\n",
    "$E(X) = \\int_{-\\infty}^{\\infty} x f(x) dx$\n",
    "\n",
    "where the integral is taken over the entire range of $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Properties of expectation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation has several important properties that make it a useful tool for analyzing random variables:\n",
    "\n",
    "1. **Linearity of expectation**: For any two random variables $X$ and $Y$ and constants $a$ and $b$, the expected value of the linear combination $aX + bY$ is given by:\n",
    "\n",
    "   $E(aX + bY) = aE(X) + bE(Y)$\n",
    "\n",
    "   This property holds even if $X$ and $Y$ are not independent.\n",
    "\n",
    "2. **Expectation of a constant**: For a constant $c$, the expected value is the constant itself:\n",
    "\n",
    "   $E(c) = c$\n",
    "\n",
    "3. **Expectation of a sum**: For any two random variables $X$ and $Y$, the expected value of their sum is the sum of their individual expected values:\n",
    "\n",
    "   $E(X + Y) = E(X) + E(Y)$\n",
    "\n",
    "   This property can be extended to the sum of any finite number of random variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_'></a>[Examples and calculations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple example to illustrate the calculation of expectation for a discrete random variable.\n",
    "\n",
    "Suppose we have a fair six-sided die. The random variable $X$ represents the number of dots on the top face of the die after a single roll. The probability of each outcome is $\\frac{1}{6}$. The expected value of $X$ can be calculated as follows:\n",
    "\n",
    "$E(X) = \\sum_{x=1}^{6} x P(X = x) = 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6} = \\frac{21}{6} = 3.5$\n",
    "\n",
    "Therefore, the expected value of the number of dots on a fair six-sided die is 3.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a Python code snippet to calculate the expected value of a discrete random variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected value is: 3.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Possible values of the random variable\n",
    "values = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Corresponding probabilities\n",
    "probabilities = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
    "\n",
    "# Calculate the expected value\n",
    "expected_value = np.sum(values * probabilities)\n",
    "\n",
    "print(f\"The expected value is: {expected_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates how to calculate the expected value of a discrete random variable using NumPy arrays to store the values and probabilities and the `np.sum()` function to compute the weighted sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Variance](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance is a measure of the dispersion or spread of a random variable around its expected value. It quantifies how much the values of the random variable deviate from the mean. A high variance indicates that the values are spread out over a wide range, while a low variance suggests that the values are clustered closely around the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Definition of variance](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of a random variable $X$, denoted as $Var(X)$ or $\\sigma^2$, is the expected value of the squared deviation of $X$ from its mean. In other words, it is the average of the squared differences between each value of $X$ and the expected value $E(X)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Notation and formula for discrete and continuous random variables](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a discrete random variable $X$ with probability mass function $P(X = x)$ and expected value $E(X)$, the variance is given by:\n",
    "\n",
    "$Var(X) = E[(X - E(X))^2] = \\sum_{x} (x - E(X))^2 P(X = x)$\n",
    "\n",
    "where the sum is taken over all possible values of $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a continuous random variable $X$ with probability density function $f(x)$ and expected value $E(X)$, the variance is given by:\n",
    "\n",
    "$Var(X) = E[(X - E(X))^2] = \\int_{-\\infty}^{\\infty} (x - E(X))^2 f(x) dx$\n",
    "\n",
    "where the integral is taken over the entire range of $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[Properties of variance](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance has several important properties that make it a useful tool for analyzing the spread of a random variable:\n",
    "\n",
    "1. **Non-negativity**: Variance is always non-negative, i.e., $Var(X) \\geq 0$, because it is the expected value of squared deviations, which are always non-negative.\n",
    "\n",
    "2. **Variance of a constant**: For a constant $c$, the variance is zero:\n",
    "\n",
    "   $Var(c) = 0$\n",
    "\n",
    "3. **Scaling property**: For a random variable $X$ and a constant $a$, the variance of $aX$ is given by:\n",
    "\n",
    "   $Var(aX) = a^2 Var(X)$\n",
    "\n",
    "4. **Variance of a sum**: For any two independent random variables $X$ and $Y$, the variance of their sum is the sum of their individual variances:\n",
    "\n",
    "   $Var(X + Y) = Var(X) + Var(Y)$\n",
    "\n",
    "   This property can be extended to the sum of any finite number of independent random variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Examples and calculations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the same example of a fair six-sided die from the previous section. The random variable $X$ represents the number of dots on the top face of the die after a single roll. We calculated the expected value $E(X) = 3.5$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the variance, we first compute the squared deviations from the mean for each possible value of $X$:\n",
    "\n",
    "$(1 - 3.5)^2 = (-2.5)^2 = 6.25$\n",
    "$(2 - 3.5)^2 = (-1.5)^2 = 2.25$\n",
    "$(3 - 3.5)^2 = (-0.5)^2 = 0.25$\n",
    "$(4 - 3.5)^2 = (0.5)^2 = 0.25$\n",
    "$(5 - 3.5)^2 = (1.5)^2 = 2.25$\n",
    "$(6 - 3.5)^2 = (2.5)^2 = 6.25$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the variance by taking the weighted average of these squared deviations:\n",
    "\n",
    "$Var(X) = \\sum_{x=1}^{6} (x - 3.5)^2 P(X = x) = 6.25 \\cdot \\frac{1}{6} + 2.25 \\cdot \\frac{1}{6} + 0.25 \\cdot \\frac{1}{6} + 0.25 \\cdot \\frac{1}{6} + 2.25 \\cdot \\frac{1}{6} + 6.25 \\cdot \\frac{1}{6} = \\frac{35}{12} \\approx 2.92$\n",
    "\n",
    "Therefore, the variance of the number of dots on a fair six-sided die is approximately 2.92.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a Python code snippet to calculate the variance of a discrete random variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance is: 2.92\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Possible values of the random variable\n",
    "values = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Corresponding probabilities\n",
    "probabilities = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
    "\n",
    "# Calculate the expected value\n",
    "expected_value = np.sum(values * probabilities)\n",
    "\n",
    "# Calculate the variance\n",
    "variance = np.sum((values - expected_value)**2 * probabilities)\n",
    "\n",
    "print(f\"The variance is: {variance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates how to calculate the variance of a discrete random variable using NumPy arrays. We first calculate the expected value, then compute the squared deviations from the mean, and finally take the weighted average of these squared deviations to obtain the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Standard Deviation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation is another measure of the dispersion or spread of a random variable, derived from the variance. It is often denoted by the Greek letter $\\sigma$ (sigma) and is expressed in the same units as the random variable itself, making it more interpretable than variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Definition of standard deviation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation of a random variable $X$, denoted as $\\sigma$ or $SD(X)$, is the square root of its variance $Var(X)$. Mathematically, it is defined as:\n",
    "\n",
    "$\\sigma = \\sqrt{Var(X)}$\n",
    "\n",
    "where $Var(X)$ is the variance of the random variable $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[Relationship between variance and standard deviation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation is directly related to the variance, as it is the square root of the variance. This relationship can be expressed as:\n",
    "\n",
    "$Var(X) = \\sigma^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation has some useful properties that are derived from the properties of variance:\n",
    "\n",
    "1. **Non-negativity**: Standard deviation is always non-negative, i.e., $\\sigma \\geq 0$, because it is the square root of the non-negative variance.\n",
    "\n",
    "2. **Scaling property**: For a random variable $X$ and a constant $a$, the standard deviation of $aX$ is given by:\n",
    "\n",
    "   $SD(aX) = |a| SD(X)$\n",
    "\n",
    "   Note that the standard deviation is affected by the absolute value of the scaling factor, unlike variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Interpreting standard deviation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation provides a measure of how much the values of a random variable typically deviate from the mean. Some key interpretations of standard deviation include:\n",
    "\n",
    "1. **Spread**: A higher standard deviation indicates that the values of the random variable are more spread out from the mean, while a lower standard deviation suggests that the values are more tightly clustered around the mean.\n",
    "\n",
    "2. **Units**: Standard deviation is expressed in the same units as the random variable, making it more intuitive to interpret than variance, which is in squared units.\n",
    "\n",
    "3. **Empirical rule** (also known as the 68-95-99.7 rule): For a random variable with a bell-shaped (normal) distribution, approximately:\n",
    "   - 68% of the values fall within one standard deviation of the mean.\n",
    "   - 95% of the values fall within two standard deviations of the mean.\n",
    "   - 99.7% of the values fall within three standard deviations of the mean.\n",
    "\n",
    "   This rule provides a quick way to estimate the proportion of values within certain ranges of the mean, based on the standard deviation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with the example of a fair six-sided die. We previously calculated the variance $Var(X) \\approx 2.92$. To find the standard deviation, we simply take the square root of the variance:\n",
    "\n",
    "$\\sigma = \\sqrt{Var(X)} \\approx \\sqrt{2.92} \\approx 1.71$\n",
    "\n",
    "Therefore, the standard deviation of the number of dots on a fair six-sided die is approximately 1.71.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a Python code snippet to calculate the standard deviation of a discrete random variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard deviation is: 1.71\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Possible values of the random variable\n",
    "values = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Corresponding probabilities\n",
    "probabilities = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
    "\n",
    "# Calculate the variance\n",
    "variance = np.sum((values - np.mean(values))**2 * probabilities)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "standard_deviation = np.sqrt(variance)\n",
    "\n",
    "print(f\"The standard deviation is: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates how to calculate the standard deviation by first computing the variance and then taking its square root using the `np.sqrt()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Covariance and Correlation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance and correlation are two related concepts that measure the relationship between two random variables. While covariance measures the direction and strength of the linear relationship between two variables, correlation is a standardized version of covariance that allows for easier interpretation and comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[Definition of covariance](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance is a measure of the joint variability of two random variables. It quantifies how much two variables change together. A positive covariance indicates that the variables tend to move in the same direction, while a negative covariance suggests that they move in opposite directions. A covariance of zero implies that there is no linear relationship between the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[Formula for calculating covariance](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance between two random variables $X$ and $Y$, denoted as $Cov(X, Y)$ or $\\sigma_{XY}$, is defined as:\n",
    "\n",
    "$Cov(X, Y) = E[(X - E(X))(Y - E(Y))]$\n",
    "\n",
    "where $E(X)$ and $E(Y)$ are the expected values of $X$ and $Y$, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sample of $n$ pairs of observations $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$, the sample covariance is calculated as:\n",
    "\n",
    "$Cov(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the sample means of $X$ and $Y$, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_'></a>[Definition of correlation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is a standardized version of covariance that measures the strength and direction of the linear relationship between two random variables. The correlation coefficient, denoted as $\\rho$ (rho) for population and $r$ for sample, is always between -1 and 1. A correlation of 1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 suggests no linear relationship.\n",
    "\n",
    "**Note**: There are several types of correlation coefficients, such as Pearson's correlation, Spearman's rank correlation, and Kendall's tau. In this lecture, we focus on Pearson's correlation coefficient, which is the most commonly used measure of linear relationship between two variables. Pearson's correlation assumes that the variables are normally distributed and have a linear relationship.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_'></a>[Formula for calculating correlation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson's correlation coefficient between two random variables $X$ and $Y$ is defined as:\n",
    "\n",
    "$\\rho = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y}$\n",
    "\n",
    "where $\\sigma_X$ and $\\sigma_Y$ are the standard deviations of $X$ and $Y$, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sample of $n$ pairs of observations $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$, the sample Pearson's correlation coefficient is calculated as:\n",
    "\n",
    "$r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_5_'></a>[Interpreting covariance and correlation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance and correlation provide insights into the relationship between two random variables:\n",
    "\n",
    "1. **Direction**: A positive covariance or correlation indicates a positive relationship (the variables move in the same direction), while a negative value suggests a negative relationship (the variables move in opposite directions).\n",
    "\n",
    "2. **Strength**: The magnitude of the covariance is not easily interpretable, as it depends on the scales of the variables. In contrast, the correlation coefficient is standardized and ranges from -1 to 1, making it easier to compare the strength of the relationship between different pairs of variables. A correlation closer to -1 or 1 indicates a stronger linear relationship, while a correlation closer to 0 suggests a weaker linear relationship.\n",
    "\n",
    "3. **Linearity**: Pearson's covariance and correlation only measure the linear relationship between variables. They do not capture non-linear relationships or dependencies. If the relationship between the variables is not linear, other measures of association, such as Spearman's rank correlation or Kendall's tau, may be more appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a Python code snippet to calculate the Pearson's correlation coefficient between two variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's correlation is: 0.99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 5, 6, 8])\n",
    "\n",
    "# Calculate Pearson's correlation\n",
    "correlation = np.corrcoef(x, y)[0][1]\n",
    "\n",
    "print(f\"Pearson's correlation is: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates how to calculate Pearson's correlation coefficient using NumPy's `np.corrcoef()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Applications in Machine Learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation, variance, covariance, and correlation play crucial roles in various aspects of machine learning. These concepts are used in data preprocessing, feature selection, model building, and evaluation. Understanding and leveraging these statistical measures can help improve the performance and interpretability of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_'></a>[Role of expectation and variance in machine learning](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data normalization**: Expectation (mean) and variance are often used to normalize or standardize features in a dataset. Normalizing features involves subtracting the mean and dividing by the standard deviation, which centers the data around zero and scales it to have unit variance. This preprocessing step can improve the convergence and performance of many machine learning algorithms, particularly those based on gradient descent optimization.\n",
    "\n",
    "2. **Feature selection**: Variance can be used as a criterion for feature selection. Features with low variance (i.e., features that have similar values across all instances) may not be informative for learning and can be removed from the dataset. This helps reduce the dimensionality of the data and can improve model performance by focusing on the most relevant features.\n",
    "\n",
    "3. **Regularization**: Regularization techniques, such as L1 (Lasso) and L2 (Ridge) regularization, are used to control the complexity of models and prevent overfitting. These techniques add a penalty term to the loss function based on the magnitude of the model's weights. The penalty term is proportional to the variance of the weights, encouraging the model to have smaller weights and reducing its sensitivity to noise in the training data.\n",
    "\n",
    "4. **Bayesian inference**: In Bayesian machine learning, prior and posterior distributions are used to represent the uncertainty in model parameters. The expectation (mean) of the posterior distribution is often used as a point estimate of the parameters, while the variance of the posterior captures the uncertainty in the estimates. Bayesian methods, such as Gaussian Processes and Bayesian Neural Networks, leverage these concepts to provide probabilistic predictions and quantify uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_'></a>[Examples of how these concepts are used in various algorithms](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Principal Component Analysis (PCA)**: PCA is a dimensionality reduction technique that finds the directions of maximum variance in the data. It uses the covariance matrix of the features to identify the principal components, which are linear combinations of the original features that capture the most variance. By projecting the data onto a subset of the principal components, PCA can reduce the dimensionality of the data while preserving the most important information.\n",
    "\n",
    "2. **Linear Regression**: In linear regression, the goal is to find the best-fitting line that minimizes the sum of squared residuals. The coefficients of the linear model are estimated using the expectation (mean) of the target variable and the covariance between the features and the target. The variance of the residuals is used to assess the goodness of fit and to estimate the uncertainty in the predictions.\n",
    "\n",
    "3. **Gaussian Naive Bayes**: Gaussian Naive Bayes is a probabilistic classifier that assumes the features follow a Gaussian (normal) distribution within each class. The model estimates the mean and variance of each feature for each class from the training data. During prediction, the model uses these estimates to calculate the likelihood of an instance belonging to each class, and then selects the class with the highest posterior probability.\n",
    "\n",
    "4. **K-Means Clustering**: K-Means is a popular clustering algorithm that aims to partition the data into K clusters based on the similarity of the instances. The algorithm iteratively assigns instances to the nearest cluster centroid (mean) and updates the centroids based on the assigned instances. The variance within each cluster is minimized, while the variance between clusters is maximized, leading to compact and well-separated clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few examples of how expectation, variance, covariance, and correlation are used in machine learning. These concepts are fundamental to many other algorithms, such as decision trees, random forests, support vector machines, and deep learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding and leveraging these statistical measures, machine learning practitioners can preprocess data effectively, select informative features, build robust models, and interpret the results in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we explored the fundamental concepts of expectation, variance, standard deviation, covariance, and correlation. These statistical measures are essential for understanding and analyzing the properties of random variables and the relationships between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_1_'></a>[Recap of key points](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Expectation** (mean) is the average value of a random variable over a large number of trials or observations. It represents the central tendency of the variable.\n",
    "\n",
    "2. **Variance** measures the dispersion or spread of a random variable around its mean. It quantifies how much the values of the variable deviate from the expectation.\n",
    "\n",
    "3. **Standard deviation** is the square root of the variance and provides a more interpretable measure of dispersion in the same units as the random variable.\n",
    "\n",
    "4. **Covariance** measures the joint variability of two random variables and indicates the direction of their linear relationship. A positive covariance suggests a positive relationship, while a negative covariance implies a negative relationship.\n",
    "\n",
    "5. **Correlation** is a standardized version of covariance that measures the strength and direction of the linear relationship between two variables. Pearson's correlation coefficient ranges from -1 to 1, with values closer to -1 or 1 indicating a stronger linear relationship.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_2_'></a>[Importance of understanding expectation and variance in machine learning](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding expectation, variance, and related concepts is crucial in machine learning for several reasons:\n",
    "\n",
    "1. These measures are used in data preprocessing, such as feature normalization and standardization, which can improve the performance and convergence of machine learning algorithms.\n",
    "\n",
    "2. Variance and covariance are employed in feature selection techniques to identify informative features and reduce the dimensionality of the data.\n",
    "\n",
    "3. Regularization methods, such as L1 and L2 regularization, use the variance of model weights to control model complexity and prevent overfitting.\n",
    "\n",
    "4. Bayesian machine learning techniques leverage expectation and variance to quantify uncertainty in model parameters and predictions.\n",
    "\n",
    "5. Many machine learning algorithms, such as linear regression, Gaussian Naive Bayes, and K-Means clustering, rely on these statistical measures for parameter estimation and decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By gaining a solid understanding of expectation, variance, and related concepts, machine learning practitioners can make informed decisions in data preprocessing, feature selection, model building, and interpretation of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_3_'></a>[Resources for further study](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To dive deeper into the topics covered in this lecture and explore their applications in machine learning, consider the following resources:\n",
    "\n",
    "1. [Expectation, Variance and Standard Deviation](https://www.youtube.com/watch?v=SzZ6GpcfoQY) by Khan Academy\n",
    "2. [Covariance and Correlation](https://www.youtube.com/watch?v=qtaqvPAeEJY) by Khan Academy\n",
    "3. [A Gentle Introduction to Expectation, Variance, and Covariance with NumPy](https://machinelearningmastery.com/introduction-to-expected-value-variance-and-covariance/) by Jason Brownlee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These resources provide clear explanations and practical examples of expectation, variance, covariance, correlation, and their applications in machine learning. They include a mix of video lectures and written articles to cater to different learning preferences and help you reinforce your understanding of these essential concepts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
